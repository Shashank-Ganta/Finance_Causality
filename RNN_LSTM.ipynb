{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow/Keras: 2.11.0\n",
      "pandas: 1.3.5\n",
      "numpy: 1.21.5\n",
      "sklearn: 1.0.2\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow / Keras\n",
    "from tensorflow import keras # for building Neural Networks\n",
    "print('Tensorflow/Keras: %s' % keras.__version__) # print version\n",
    "from keras.models import Sequential # for creating a linear stack of layers for our Neural Network\n",
    "from keras import Input # for instantiating a keras tensor\n",
    "from keras.layers import Dense, SimpleRNN # for creating regular densely-connected NN layers and RNN layers\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd # for data manipulation\n",
    "print('pandas: %s' % pd.__version__) # print version\n",
    "import numpy as np # for data manipulation\n",
    "print('numpy: %s' % np.__version__) # print version\n",
    "import math # to help with data reshaping of the data\n",
    "\n",
    "# Sklearn\n",
    "import sklearn # for model evaluation\n",
    "print('sklearn: %s' % sklearn.__version__) # print version\n",
    "from sklearn.model_selection import train_test_split # for splitting the data into train and test samples\n",
    "from sklearn.metrics import mean_squared_error # for model evaluation metrics\n",
    "from sklearn.preprocessing import MinMaxScaler # for feature scaling\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quandl\n",
    "import numpy as np\n",
    "import csv \n",
    "\n",
    "quandl.ApiConfig.api_key = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_data = quandl.Dataset('WIKI/AAPL').data(params={ 'start_date':'2010-01-01', 'end_date':'2017-01-01', 'collapse':'weekly', 'transformation':'rdiff', 'rows':1000 })\n",
    "test_dataset_data = quandl.Dataset('WIKI/AAPL').data(params={ 'start_date':'2017-01-01', 'end_date':'2018-01-01', 'collapse':'weekly', 'transformation':'rdiff', 'rows':1000 })\n",
    "total_dataset_data = quandl.Dataset('WIKI/AAPL').data(params={ 'start_date':'2010-01-01', 'end_date':'2018-01-01', 'collapse':'weekly', 'transformation':'rdiff', 'rows':1000 })\n",
    "\n",
    "\n",
    "train_np = train_dataset_data.to_numpy()\n",
    "test_np = test_dataset_data.to_numpy()\n",
    "total_np = total_dataset_data.to_numpy()\n",
    "\n",
    "\n",
    "train_data = [[\"date\",'adj_open','adj_high','adj_low','adj_close','adj_volume']]\n",
    "test_data = [[\"date\",'adj_open','adj_high','adj_low','adj_close','adj_volume']]\n",
    "total_data = [[\"date\",'adj_open','adj_high','adj_low','adj_close','adj_volume']]\n",
    "\n",
    "for date in train_np:\n",
    "    train_data.append([str(date[0])[:10],date[8],date[9],date[10],date[11],date[12]])\n",
    "for date in test_np:\n",
    "    test_data.append([str(date[0])[:10],date[8],date[9],date[10],date[11],date[12]])\n",
    "for date in total_np:\n",
    "    total_data.append([str(date[0])[:10],date[8],date[9],date[10],date[11],date[12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write data gathered into CSV files\n",
    "with open(\"./APPL_train.csv\", 'w') as csvfile: \n",
    "    # creating a csv writer object \n",
    "    csvwriter = csv.writer(csvfile,lineterminator = '\\n') \n",
    "    csvwriter.writerows(train_data)\n",
    "with open(\"./APPL_test.csv\", 'w') as csvfile: \n",
    "    # creating a csv writer object \n",
    "    csvwriter = csv.writer(csvfile, lineterminator = '\\n') \n",
    "    csvwriter.writerows(test_data)\n",
    "with open(\"./APPL_total.csv\", 'w') as csvfile: \n",
    "    # creating a csv writer object \n",
    "    csvwriter = csv.writer(csvfile, lineterminator = '\\n') \n",
    "    csvwriter.writerows(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits samples based off of months\n",
    "\n",
    "train_samples_index = []\n",
    "for index in range(len(train_data)-2):\n",
    "    date_current = train_data[index+1][0]\n",
    "    date_next = train_data[index+2][0]\n",
    "    if date_current[:7] == date_next[:7]:\n",
    "        print(\"same\")\n",
    "    else:\n",
    "        train_samples_index.append(index+1)\n",
    "        print(index+1)\n",
    "\n",
    "test_samples_index = []\n",
    "for index in range(len(test_data)-2):\n",
    "    date_current = test_data[index+1][0]\n",
    "    date_next = test_data[index+2][0]\n",
    "    if date_current[:7] == date_next[:7]:\n",
    "        print(\"same\")\n",
    "    else:\n",
    "        test_samples_index.append(index+1)\n",
    "        print(index+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets indexes for samples based off grouping of 4weeks input, 1 week output\n",
    "\n",
    "train_samples_index = []\n",
    "for i in range(len(train_data) - 1):\n",
    "    if i % 5 == 0 and i != 0:\n",
    "        train_samples_index.append(i)\n",
    "\n",
    "test_samples_index = []\n",
    "for i in range(len(test_data) - 1):\n",
    "    if i % 5 == 0 and i != 0:\n",
    "        test_samples_index.append(i)\n",
    "\n",
    "total_samples_index = []\n",
    "for i in range(len(total_data) - 1):\n",
    "    if i % 5 == 0 and i != 0:\n",
    "        total_samples_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#splits samples based on indexes \n",
    "\n",
    "train_samples = []\n",
    "start_index = 1\n",
    "end_index = 1\n",
    "for indexes in train_samples_index:\n",
    "    end_index = indexes + 1\n",
    "    train_samples.append(train_data[start_index:end_index])\n",
    "    start_index = end_index\n",
    "\n",
    "total_samples = []\n",
    "start_index = 1\n",
    "end_index = 1\n",
    "for indexes in total_samples_index:\n",
    "    end_index = indexes + 1\n",
    "    total_samples.append(total_data[start_index:end_index])\n",
    "    start_index = end_index\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#make X and y data ~ X (stock data) ~ y (resulting week's open/close)\n",
    "\n",
    "X_data = []\n",
    "y_data = []\n",
    "goal = \"close_price\"\n",
    "for sample in total_samples:\n",
    "    #remove \"date\" from X_data\n",
    "    stock_data = sample[:-1]\n",
    "    revised_data = []\n",
    "    open = []\n",
    "    high = []\n",
    "    low = []\n",
    "    close = []\n",
    "    volume = []\n",
    "    for date in stock_data:\n",
    "        open.append(date[1])\n",
    "        high.append(date[2])\n",
    "        low.append(date[3])\n",
    "        close.append(date[4])\n",
    "        volume.append(date[5])\n",
    "\n",
    "    avg_open = np.average(open)\n",
    "    avg_high = np.average(high)\n",
    "    avg_low = np.average(low)\n",
    "    avg_close = np.average(close)\n",
    "    avg_volume = np.average(volume)\n",
    "\n",
    "    week_data = [avg_open,avg_high,avg_low,avg_close,avg_volume]\n",
    "    X_data.append(week_data)\n",
    "    \n",
    "    if goal == \"open_price\":\n",
    "        y_data.append(float(sample[-1][1]))\n",
    "    elif goal == \"close_price\":\n",
    "        y_data.append(float(sample[-1][4]))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#gets LOOCV indexes for train and test sets \n",
    "\n",
    "LOOCV_indexes = []\n",
    "for i in range(len(X_data)):\n",
    "    train_index = []\n",
    "    for x in range(len(X_data)):\n",
    "        if i != x:\n",
    "            train_index.append(x)\n",
    "    LOOCV_indexes.append((train_index,i))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1805379479950512\n",
      "-0.0894484035942062\n",
      "0.01656581365015734\n",
      "0.025209665029444946\n",
      "-0.03550209127690363\n",
      "-0.04039172223737765\n",
      "-0.05762964824017409\n",
      "-0.09089549367657378\n",
      "0.0461011242757215\n",
      "-0.03590480972693856\n",
      "0.10727371395201321\n",
      "-0.006631084073566009\n",
      "0.008103081328596672\n",
      "-0.0361480788240542\n",
      "0.09988416752080308\n",
      "-0.10777070123854414\n",
      "0.12327187704816564\n",
      "-0.137426901339377\n",
      "0.026590239616176094\n",
      "-0.006188532692199492\n",
      "0.02502570701246999\n",
      "-0.09503409579532388\n",
      "-0.22188222198557953\n",
      "0.03713732940993011\n",
      "-0.05631289199768337\n",
      "-0.039775584516180275\n",
      "-0.0772181194372331\n",
      "-0.031348381403516326\n",
      "0.07217445536688222\n",
      "0.06550840435064144\n",
      "0.05139309786046097\n",
      "0.206361794272836\n",
      "0.11358782444575295\n",
      "0.04018284901818687\n",
      "-0.07351612861084855\n",
      "0.06780987997577227\n",
      "0.04996126204263258\n",
      "-0.11439347695029282\n",
      "-0.013287478629031717\n",
      "-0.08413525264325929\n",
      "0.025510252343555265\n",
      "0.06986504201059647\n",
      "-0.01457081622758076\n",
      "0.05641815382210048\n",
      "-0.042571912162653824\n",
      "-0.03402348367416721\n",
      "-0.033367588669337825\n",
      "0.0012010151916563932\n",
      "-0.020961245448957392\n",
      "0.040463069753347146\n",
      "-0.07316513557047429\n",
      "0.016949451928936352\n",
      "-0.05275774339000397\n",
      "0.012226732004668873\n",
      "-0.0004550756354700553\n",
      "-0.022319423438814722\n",
      "0.0010445395384723077\n",
      "0.01844804861542048\n",
      "-0.0204990018516571\n",
      "0.0352377154543051\n",
      "-0.06548620109530319\n",
      "0.04823413074708234\n",
      "0.16435881741766684\n",
      "0.0019015523852742475\n",
      "-0.01940961501056066\n",
      "0.22206258565972725\n",
      "-0.005067988149942789\n",
      "0.006539536996222279\n",
      "-0.03428935911726642\n",
      "-0.013759964082990468\n",
      "-0.021503515199347425\n",
      "-0.06927377651329834\n",
      "0.052226719982594955\n",
      "-0.05149786504909926\n",
      "0.023803507803525196\n",
      "0.06848296977419267\n",
      "-0.025338449879602314\n",
      "0.0364230126155856\n",
      "0.012861773988908034\n",
      "-0.0420304026203087\n",
      "0.051869169406182754\n",
      "-0.0732082817898803\n",
      "0.0030982156955252364\n"
     ]
    }
   ],
   "source": [
    "#runs Gradient Boosted Regressor and gets difference values for each stock\n",
    "\n",
    "total_diff = []\n",
    "total_result = []\n",
    "for fold in LOOCV_indexes:\n",
    "    #gets correct samples for LOOCV train & test\n",
    "    clf_train_X_data = []\n",
    "    clf_train_y_data = []\n",
    "    clf_test_X_data = []\n",
    "    clf_test_y_data = []\n",
    "    for index in fold[0]:\n",
    "        clf_train_X_data.append(X_data[index])\n",
    "        clf_train_y_data.append(y_data[index])\n",
    "    clf_test_X_data.append(X_data[fold[1]])\n",
    "    clf_test_y_data.append(y_data[fold[1]])\n",
    "\n",
    "    clf = GradientBoostingClassifier(n_estimators=10, learning_rate=0.25,max_depth=1, random_state=0)\n",
    "    clf = GradientBoostingRegressor(random_state=0)\n",
    "\n",
    "    clf.fit(clf_train_X_data,clf_train_y_data)\n",
    "\n",
    "    predicted_value = clf.predict(clf_test_X_data)[0]\n",
    "    actual_value = clf_test_y_data[0]\n",
    "\n",
    "    difference = predicted_value - actual_value\n",
    "    difference_precentage = difference / actual_value\n",
    "    total_diff.append(difference_precentage)\n",
    "    total_result.append([predicted_value,actual_value,difference_precentage])\n",
    "    print(difference_precentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_diff: 0.0017531120944778407\n",
      "avg_over_estimate: 0.0568757621294449\n",
      "avg_under_estimate: -0.05205709389203764\n"
     ]
    }
   ],
   "source": [
    "avg_diff = np.average(total_diff)\n",
    "overEstimate = []\n",
    "underEstimate = []\n",
    "\n",
    "for diff in total_diff:\n",
    "    if diff > 0:\n",
    "        overEstimate.append(diff)\n",
    "    else:\n",
    "        underEstimate.append(diff)\n",
    "\n",
    "avg_over_diff = np.average(overEstimate)\n",
    "avg_under_diff = np.average(underEstimate)\n",
    "print(\"avg_diff: \" + str(avg_diff))\n",
    "print(\"avg_over_estimate: \" + str(avg_over_diff))\n",
    "print(\"avg_under_estimate: \" + str(avg_under_diff))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
